\documentclass[11pt]{article}
\usepackage[letterpaper,text={7in,9in},centering]{geometry}
\usepackage{tocloft}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsfonts,amsmath,amssymb,amsthm}
\usepackage{array}
\usepackage{fancyhdr,fancyvrb}
\usepackage[pdftex]{graphicx}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{natbib}
\usepackage[lofdepth,lotdepth]{subfig}
\usepackage{sectsty}
\usepackage{setspace}
\usepackage{stackrel}
\usepackage{xspace}

\pagestyle{fancy}
\fancyhead[RO]{Edward Gan \& Max Wang}

\sectionfont{\large}
\subsectionfont{\normalsize}
\subsubsectionfont{\normalfont\itshape}

%\setlength{\cftbeforesecskip}{0pt}

\newcommand{\Sequitur}{\textsc{Sequitur}\xspace}
\newcommand{\Similarity}{\textsc{Similarity}\xspace}
\newcommand{\Cluster}{\textsc{Cluster}\xspace}

\lstset{basicstyle=\ttfamily\footnotesize,breaklines=true}

\begin{document}
\bibliographystyle{plain}

\title{%
  \vspace{-.2in}%
  Squint: Lossy Hierarchical Compression on Symbol Streams%
  \vspace{-.1in}%
}
\author{Edward Gan \& Max Wang}
\date{%
  \vspace{-.1in}%
  December 11, 2012%
}

\maketitle

\begin{abstract}

Grammar-based compression schemes like \Sequitur exploit repeated structures in
streams of symbolic data to achieve good compression ratios.  We develop a set
of algorithms for improving upon grammar-based compression schemes by allowing
some lossiness in the compression.  Although precise subsequences might be
altered, in return we can simplify the grammars produced to obtain better
ratios while still preserving important syntatic patterns at both the micro and
macro levels.

\end{abstract}

\setcounter{tocdepth}{2}
\tableofcontents

\section{Introduction and Goals}

Many sources of data exhibit natural hierarchical structure.  For instance,
English text often can be sensibly grouped into sections, paragraphs,
sentences, and so on, all the way down to words and phonemes.  Likewise, music
consists of repeated motifs, phrases, themes, etc., which make up musical
structure.  Programming languages are so hierarchical that their syntax can
usually be represented in BNF form.  It is natural, then, to represent these
types of data using Context-Free Grammars (CFGs), which capture these
hierarchical structures; this form of \emph{grammar-based compression} can be
used to achieve very good compression ratios.

In the parallel world of image compression, however, two different classes of
compression schemes exist---\emph{lossless compression} and \emph{lossy
compression}.  In lossy compression schemes we allow a certain amount of 
corruption in order to achieve better
compression ratios.  Some situations may demand lossless encoding, but many others
have less stringent requirements. With texture
patterns and some photographs for instance, the exact pixel values are far less important
than the overall gradients, colors, and structures in the image.  In such
cases, lossy compression schemes, such as JPEG for images \cite{jpeg}, can be
valuable and effective.

We believe it is worthwhile to introduce these notions of lossy compression
that are prevalent in image/video compression into the world of hierarchical
symbol-stream compression. Introducing lossiness to symbol-stream compression 
opens up the possibility for better compression ratios.

It may seem strange at first to allow a string such as "mind" to be lossily
corrupted into, say, ``rind''.  If, however, one is more interested in
recurrent structure in the text than in its precise semantic meaning, these
corruptions are unimportant.  For instance, when looking at text in a foreign
language, one is more likely to pick up the texture of the passage, the
repeated sounds, common phrases, and breaks between larger-scale sections of
text.  In this situation, a change of a single letter would probably go
unnoticed.  Similarly, when skimming through a huge file, say, an execution
log, an administrator usually begins by looking for patterns that stand out.
Lorem ipsum is a perfect example of a piece of ``text'' whose value lies in its
texture and sentence/paragraph structure, and we will return to it in later
sections.

In such situations, lossy hierarchical compression schemes may prove beneficial
if they can provide better compression ratios.  Moreover, introducing lossiness
to the CFGs which represent the input stream may accentuate more important
parts of the stream's structure.  Compression of a musical passage might, for
instance, capture important motives in favor of showing every rhythm and note.
Similarly, compression of text in a particular language may highlight
syntactical features.  We consider this a relevant potential use of lossy
hierarchical compression.

In this paper, we develop two algorithms and implement a broader system for
lossily compressing text while preserving its hierarchical structures.  The
goal was to preserve the feel of the text at both micro and macro levels.
Since grammar-based compression schemes were designed to capture these
structures, we built our system on top of the existing \Sequitur grammar
inference algorithm.  Starting with the CFG generated by a lossless grammar
inference algorithm, we repeatedly simplify it, throwing out details along the
way to emphasize repetitions.  Our main contribution lies in the development,
implementation, and evaluation of two algorithms for ``simplifying'' CFG
grammars while attempting to preserve their structure.

\section{Background}

The work in this paper builds upon existing research in lossless grammar based
compression, and also draws inspiration from many techniques used in lossy
image and video compression.

\subsection{Grammar Compression}

As described in the introduction, the proposed research shares many of its
goals with the \Sequitur algorithm described in \cite{sequitur}.  The authors
of \cite{sequitur} develop an effective algorithm based on iteratively
rewriting grammars to keep them small and efficient, and produce interesting
grammar-based analyses of texts and musical scores.

The same author in \cite{nevillphd} explores a variety of ad-hoc extensions to
the \Sequitur algorithm which improve its compression performance on structured
data.  These ranged from introducing domain-specific constraints to their
grammar, adding a few steps of backtracking to their normally greedy grammar
formation, to guessing unifications in attempting to infer recursive grammar
rules.  Though these may invalidate some of their theoretical results on the
asymptotic performance of \Sequitur, in practice they seem to have worked and
add nicely to the grammar inference framework.

The intuition that grammar inference can support many detailed policies is made
more formal in \cite{grammarcodes}.  The authors classify the properties a
grammar needs to function as a good compressor, and moreover give a set of
reduction rules for putting a grammar into an appropriate ``irreducible'' form.
Within this context, grammar inference algorithms similar to \Sequitur can be
described as simple applications of their reduction rules to different ways of
generating base grammars.  Even more examples of the variety of grammar
inference schemes possible under this general framework of reducing grammars
can be found in \cite{efficientgreedy}

In summary, established work on grammar inference algorithms provides a solid
set of tools for experimenting with the kinds of lossy modifications we
propose to make to simplify grammar structure, and we will draw upon the kinds
of grammar transformations used in \Sequitur and in Kieffer's reduction rules.

\subsection{Lossy Compression}

While we were unable to find work in the area of lossy grammar compression, a
number of existing techniques for lossy compression more broadly serve
as inspiration.

Kieffer briefly identifies two primary methods for lossy hierarchical
compression: wavelet-based and fractal-based schemes \cite{tutorial}.  In
wavelet-based schemas, a signal is compressed by recording the largest
coefficients of its wavelet decomposition, in effect trying to represent a
signal as closely as possible using wavelet building blocks.  The JPEG image
format uses a similar technique in order to compress images lossily
\cite{jpeg}.  Fractal based compression takes this a step further and
represents parts of a signal using self-similarity, encoding an image for
instance as a fixed point of contraction maps.  This is similar in spirit to
what is done in MPEG-1, where parts of one frame can be transformed to encode
parts of the next frame. Many of these ideas, such as finding largest matching
components of objects, make their way into our algorithms.

Work has also been done regarding the lossy compression of other input formats.
For text, methods such as replacing words with synonyms \cite{semanticlossy} or
reordering the middle letters of a word (i.e., all letters save for the first
and last) \cite{semilossless} have been explored.  However, these are very
specific tricks to optimize text, and not more broadly applicable to
compressing hierarchical streams.

\section{Lossifier Algorithms}

\subsection{Motivation}

In order to make our algorithm for lossily compressing streams as general as
possible, we decided to approach the problem of lossy compression by starting
with a lossless grammar inference algorithm.  By building off of a grammar
inference algorithm, we hope to take advantage of some of the structures that
are visible in a CFG representation of a stream.  In this project, we chose
\Sequitur, but any method of inferring a grammar could be used.  After
inference, we simplified the resulting grammar, introducing loss and obtaining
greater compression.  Thus our full lossy grammar compression system is a
multi-step process:

\begin{enumerate}
  \item Infer a CFG representation of a stream (we use \Sequitur).
  \item Simplify the CFG, introducing loss.
  \item Encode and output the CFG.
\end{enumerate}

Our core contribution comes in step two, via what we refer to as a
\emph{lossifier} algorithm---an algorithm that takes an admissible CFG and
structurally simplifies it.  The output CFG should be smaller than the input
CFG but generate a string which preserves the structure in the original string
as much as possible.  In this paper, we focus on implementing and evaluating
this step, and hence omit step 3 altogether.

Building off the ideas in lossy compression discussed earlier, one idea for
simplifying data is to look at a part of the data and try to match it as
closely as possible with an existing set of primitives.  In particular, just as
with MPEG and fractal compression, to approximate one part of the grammar we
can use \emph{other parts of the grammar}.  This is the core principle behind
both of our algorithms.  Given a rule $A \rightarrow \alpha_1, \ldots,
\alpha_n$, we will try to replace occurences of $A$ with other similar
production rules.

For example, consider the first grammar given in \autoref{sred}.  In this
graphical representation, we omit the ordering of symbols on the right-hand
side of production rules.  The picture makes it clear, however, that one easy
route for simplifying the grammar is to simply replace the rule B with A.  They
differ in their expansions by only a single character, `e', and they form
almost identical halves of S.

\begin{figure}[t]
\centering
\includegraphics[scale=0.6]{include/sred1.pdf}
\includegraphics[scale=0.6]{include/sred2.pdf}
\caption{Grammar Simplification Example}
\label{sred}
\end{figure}

\subsection{Pair-Similarity}

To define whether two variables are \emph{similar}, we can look at their
expansions. Let $expand(rhs)$ denote the string of terminal symbols we get when
we recursively substitute all of the variables inside $rhs$. Then $V_1
\rightarrow rhs_1$ and $V_2 \rightarrow rhs_2$ are similar when
\[
  \frac{levenshtein(expand(rhs_1),expand(rhs_2))}
      {max(len(expand(rhs_1)),len(expand(rhs_2)))} < \epsilon
\]
where $levenshtein$ is the Levenstein edit distance between two strings.  For
convenience we can define.

\[ lsim(s1,s2) = \frac{levenshtein(s1,s2)}{max(len(s1),len(s2))} \]
Our first algorithm takes the idea of variable similarity and uses it to
iteratively simplify the grammar.

\begin{algorithm}[h]
\caption{Similarity Lossifier Algorithm}
\label{sim_alg}
\begin{algorithmic}[1]
\Procedure{Lossify-Sim}{$g$} \Comment{Simplify grammar g}
  \While{there are similar variables to replace}
    \For{$var_1 \rightarrow rhs_1,\ var_2 \rightarrow rhs_2 \in g$}
      \State $str_{i} \gets expand(rhs_i)$
      \If{$lsim(str_{1},str_{2})<\epsilon$}
        \State replace the longer var with the smaller throughout $g$
        \State break
      \EndIf
    \EndFor
  \EndWhile
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Clustering}

The previous algorithm updates the grammar sequentially, since each variable
replacement can have an effect on large parts of the grammar, we will have to
go through comparing all pairs again after the change.  An alternative
algorithm would be to try to replace similar variables in parallel. This would
avoid many of the repeat pair-comparisons done in \Similarity. In other
words, the algorithm below puts similar variables into equivalence class
clusters, and then replaces each variable in a cluster them with the smallest
representative all at once each iteration.

\begin{algorithm}[h]
\caption{Cluster Lossifier Algorithm}
\label{cluster_alg}
\begin{algorithmic}[1]
\Procedure{Cluster-Sim}{$g$} \Comment{Simplify grammar g}
  \While{there are similar variables to replace}
    \State $clusters \gets ()$
    \For{$var_1 \rightarrow rhs_1 \in g$}
      \For{$c \in clusters$}
        \State $(var_2 \rightarrow rhs_2) \gets $ first variable in $c$
        \State $str_{i} \gets expand(rhs_i)$
        \If{$lsim(str_{1},str_{2})<\epsilon$}
          \State add $var_1$ to $c$
          \State break
        \EndIf
      \EndFor
      \If{$var_1$ hasn't been added to a cluster}
        \State add $var_1$ to a new cluster in $clusters$
      \EndIf
    \EndFor
    \State save a mapping $m$ from variables to sizes
    \For{Cluster $c \in clusters$}
      \State replace all $var \in c$ with the smallest (smallest w.r.t. $m$).
    \EndFor
  \EndWhile
\EndProcedure
\end{algorithmic}
\end{algorithm}

Both of the algorithms depend on the concept of the "smallest / smaller"
variable. We can define the size of a variable to be the length of the
expansion of its rhs. In this way, if we think of our CFG's as a DAG, then
variables higher up on the DAG will always have larger expansions than those
lower down, and thus will have a larger size. By always replacing with the
smallest variable, we avoid creating cyclic references in our CFG.  We will
prove this fact in the next section.

\subsection{Proof of Grammar Consistency}

One risk in replacing variables in a CFG is that one could introduce cyclical
expansion in the CFG, such as if $A\rightarrow aB$ and $B\rightarrow bA$. In
this section we will argue that neither of our algorithms do so if the original
grammar was well formed.

We can view the CFG as a DAG where for variables $V_1$, $V_2$, we add an edge
whenever $V_1$ contains $V_2$ in its rhs. When sizes are computed on a CFG
before we modify it, if $V_1 \rightarrow V_2$ then $size(V_1) > size(V_2)$ since
the rhs of $V_1$ contains $V_2$.
Replacing a variable $V_1$ with $V_2$ has the same effect as redirecting all
in-edges to $V_1$ onto $V_2$.

Suppose for the sake of contradiction that the \Similarity algorithm created a
cycle at one step by redirecting an edge originally pointing at $A$ to $B$.
This would mean that there was a path going from $B$ to $A$. However, by
comparing the sizes of $A$ and $B$ we know that $A$ must have been bigger than
$B$. Thus there cannot have been a path from $B$ to $A$, and we have a
contradiction.

For the \Cluster algorithm the analysis is a bit more involved, since to avoid
recomputation we only calculate the sizes of the variables once before doing
all of the replacements. First note that even as we modify the DAG, all of the
edges we create in the DAG will respect the ordering of sizes that we
calculated before we did any replacements. This is because each time we
redirect an edge from $X\rightarrow A$ to $X\rightarrow B$
$size(X)>size(A)>size(B)$ and transitivity will preserve the ordering.
Suppose then that we created a cycle during \Cluster by redirecting an edge from
$X \rightarrow A$ to $X \rightarrow B$.  This means that there must have been a
path from $B$ to $X$. However, even in its intermediate state we know that the
DAG respects the $size$ ordering, and $size(X) > size(A) > size(B)$, so there
cannot be a path from $B$ to $X$. This is a contradiction and we are done.

\subsection{Runtime Analysis}

Given an input of length $n$, let $r$ be the number of rules generated by
\Sequitur from the input.  In the analysis of the \Similarity algorithm, let $R$
be the total number of variable replacements we perform. For each replacement,
we had to have searched though up to all pairs of variables, and for each pair
of variables expanding them and calculating the levenshtein distance could take
time linear in $n$.  Performing the replacement only takes time linear in $r$.
Thus the runtime is $\boxed{O(R*r^2*n)}$.

From the \Sequitur paper \cite{sequitur} we know that the number of rules in
the grammar generated by \Sequitur is bounded by $O(n)$. Thus the worst
case runtime on the similarity algorithm is $\boxed{O(n^4)}$. This is because
we could have up to $r$ replacements.  However, in practice we will see that
the runtime is much better than $O(n^4)$. There are a variety of possible
reasons for this: in our test data, $r$ didn't grow quite linearly in $n$, $R$
could grow much slower than $r$, and the use of native code in calculating the
levenshtein distance made the its contribution much less noticeable for the
input sizes we tested.

The inner loops of the \Cluster are similar to those of the \Similarity
algorithm, and an analogous analysis yields a runtime of $\boxed{O(C*r^2*n)}$. 
Just as with the
similarity algorithm however, a variety of factors make the runtime much better than
the $\boxed{O(n^4)}$ worst case scenario.  The big gain over \Similarity
is that $C$ turns out to be nearly constant, since there exist very few
huge chains of reductions that must be performed in sequence
so we are able to perform many reductions in parallel.
Thus a more reasonable upper bound for the runtime of cluster would
be $\boxed{O(n^3)}$

\section{Implementation}

Our full grammar compression scheme involves the following sequence of
operations and components:
\begin{enumerate}
  \item \Sequitur, which infers a CFG from the input stream.
  \item Lossifier algorithms, which take the \Sequitur grammar and simplify it,
    introducing lossiness.
  \item Reduction engine, which applies Kieffer and Yang's reduction rules to
    the lossified grammar.
\end{enumerate}

We implement all these components in the Ruby programming language.  Our
implementation of \Sequitur exactly mirrors the data structures and algorithm
described by Nevill-Manning and Witten \cite{sequitur}---and, empirically, it
has the same asymptotic running time---so we do not discuss it further.
However, we will examine in this section some of the techniques we used in
implementing \Similarity, \Cluster, and the reduction engine.

\subsection{Data Structures and Optimizations}

\subsubsection{CFG Representation}

Our representation of a context-free grammar is very similar to that used by
\Sequitur.  All of our rules are represented by a linked list of symbols;
however, unlike in \Sequitur, we omit refcounting and do not point from each
instance of a nonterminal symbol to its corresponding rule.  Instead, our rules
are kept in a hash table, mapping nonterminal symbols to a list representing
its RHS.

We chose to implement a list library rather than use Ruby's native arrays in
order to achieve better runtime for the splice operations.  Both \Sequitur and
our implementation of Kieffer and Yang's reductions require us to splice out a
subsequence of terminals from a rule, or to insert a subsequence (e.g., as a
result of inlining a singleton rule) into a rule.

Grammar Data Structure
Linked Lists vs Arrays, Caching Expansions, Native Levenshtein

\subsubsection{Lossifier Optimizations}

Perhaps the most crucial optimization we implement deals with the recursive
expansion of nonterminals into terminal strings.

\subsection{Grammar Reductions}

What is their purpose? Why do we want them? What are they?  Grammar reductions
are a way of taking a CFG, generated possibly through ad-hoc techniques, and
systematically removing internal redundancies like duplicate bigrams in
\Sequitur. They make the CFG representation smaller but don't introduce any
loss. We want them since our lossifier algorithms often make our grammars
redundant, even though the each step we take in the lossifier makes the
grammars smaller.

\section{Results}

\subsection{Illustrative Examples}

It is useful to look at how the algorithms function on a small piece of data
with both plenty of hierarchical structure, and also some messiness that most
lossless compression schemes would not deal well with. As an example,
the string below is
similar to what one might come across in a math puzzle book.

\begin{verbatim}
123,124,123 + 321,323,321 = ???
124,123,123 + 321,3231,321 = ????
--
124,123,123 + 321,3231,321 = ????
123,124,123 + 321,323,321 = ????
\end{verbatim}

Disregarding its meaning, it consists of four very similar equations of the
form $a + b = ?$, and on a smaller scale each summand consists of 3 repetitions
of approximately the same string of either $123,$ or $321,$.

When comparing grammars, we chose to define the \emph{size} of a grammar as the
sum of the lengths of the rhs's of the production rules.  \Sequitur yields a
relatively complicated inferred grammar with 13 rules and a size of 51. Both
the \Similarity and \Cluster algorithms with $\epsilon = .4$ , yield the
following grammar after reduction. It has 5 rules and is almost half the size,
at 30.

\begin{verbatim}
Grammar:
~[*] => ~[BC]--~[BC]?

~[AZ] => ~[A]3~[A]~[A] + ~[C]~[C]3~[C]1 = ???
~[A] => 12
~[BC] => ~[AZ]~[AZ]
~[C] => 32
-----------------------
Generated String:
1231212 + 32323321 = ???1231212 + 32323321 = ???--(cont.)
1231212 + 32323321 = ???1231212 + 32323321 = ????
\end{verbatim}

The lossifier algorithm reduced the string to a pair of pairs of the same
equations, where each equation consists of the sum of triples of "12" or "32".
Smaller discrepencies in the numbers involved were dropped, while the filler
commas and linebreaks also ended up being lost. Note that the structure
described is easily observable from the grammar generated.

\subsection{Similarity vs Clustering}

\subsubsection{Methodology}

For most of our benchmark tests, we focused on two extreme kinds of data: one
with large amounts of repetition at all levels, and one with a word and
sentence level structure but with little obvious large-scale repetition.  One
set of test data was constructed by repeating the same letter `a' n-times but
then adding some texture to the data by changing 1/30 of the characters
uniformly at random to `*'. These are the \emph{rep-n} test-cases.
Another set of
test data was constructed from the first n characters of the book of Genesis
from the KJV version of the Bible. Call these test-cases \emph{gen-n}.

For our first suite of tests, we compared the effectiveness of the
\Similarity and \Cluster algorithms. We might expect \Similarity to
achieve better quality at the same compression ratio since it updates the state
of the grammar between each variable replacement, on the other hand since it
must update the grammar it has the potential to run much more slowly than
\Cluster.

In table \ref{simclus_r_t} we report the results on running \Sequitur and
\Cluster, without the grammar reductions, on the rep-n datasets. In table
\ref{simclus_g_t} we do the same for the gen-n datasets.
In these tests
we were looking to evaluate the relative fidelity, compression ratios, and runtime
of the two algorithms.

In the tables, Time is the
wall clock running as measured on a 2.26 Ghz Intel Core 2 Duo Macbook Pro.
Ratio is the relative size of the grammar produced compared with \Sequitur,
where the size of the grammar is the sum of the lengths of the rhs's of the
rules. Levdistance is the levenshtein distance between the output of the
lossified grammar and the original input string, it is a kind of measure of the
fidelity of our algorithms but doesn't take into account how well patterns were
preserved. For the rep-n tests, proportion is the proportion of the final
string produced that was `*'. The closer to 1/30~0.033, the more accurate the
lossifier was in recreating the original texture of the input.

\subsubsection{Result Summary}

In both datasets, the
compression ratios achieved for both algorithms are roughly equal given the
same value for $\epsilon$, so it is fair to compare the algorithms for the same
values of $\epsilon$.
From plotting the data, we see that runtime of the algorithms are roughly
polynomial in the size of the data, so we can perform a linear regression on
the log-log transform of the data to find the exponents $\alpha$ for the
runtime $r(n)=n^\alpha$ where $n$ is the length of the input string. In the
charts below, all of the linear regressions had $R^2$ values of over $0.98$.

\begin{tabular}{l|l|l|l|l}
rep-n runtime & Sim: $\epsilon=.4$ & Sim: $\epsilon=.2$ & Clust: $\epsilon = .4$ & Clust: $\epsilon = .2$ \\
Exponent      & $1.859$ & $2.026$ & $1.306$ & $1.305$
\end{tabular}

\begin{tabular}{l|l|l|l|l}
gen-n runtime & Sim: $\epsilon=.4$ & Sim: $\epsilon=.2$ & Clust: $\epsilon = .4$ & Clust: $\epsilon = .2$ \\
Exponent      & $2.629$ & $2.493$ & $1.814$ & $2.061$
\end{tabular}

In general, \Cluster has a runtime which is slightly better than quadratic,
while \Similarity has a runtime which is slightly worse than quadratic.
Differant values of $\epsilon$ have comparatively smaller effects on the
runtime, and both algorithms performed worse on the gen-n dataset. We think
this is because there are more tiny substitutions one can perform on
the gen-n dataset, whereas the rep-n dataset allows for much larger scale
substitutions which eliminate the need for many small substitutions.
As expected, \Cluster has a significantly better runtime than \Similarity,
but it is interesting that both algorithms performed much better than our
worst case analysis, we believe it is for the same reasons we proposed
in our runtime analysis section.


Across all of the tests, the leveshtein
distances were comparable for both algorithms. Though they differed in
individual cases, no algorithm did consistently better than the other.
In general the levenshtein distances were high, except in the $\epsilon=.2$
case for the gen-n data where very little compression was possible at all.
We believe most of the distance could be explained by
the fact that the algorithms could drastically shorten
the strings, since it always replaces variables with \emph{smaller} variables
as discussed earlier.
The proportion of '*'s in the rep-n input
was usually the right order of magnitude, with both algorithms making big
mistakes occassionally. In most cases, we felt that the two algorithms maintained similarly
acceptable fidelity levels in recreating the texture of the original rep-n
input file, and we discuss this subjective impression further in the
Larger Tests section.


The actual compression ratio was respectable in most tests, with both \Similarity and \Cluster
coming in at nearly half the size of the \Sequitur CFG for the rep-n data,
and 70 percent for the gen-n data. A notable exception was
the gen-n test with $\epsilon=.2$, where $\epsilon$ was so small that barely
any substitutions were possible.
However, setting higher values
for $\epsilon$ had diminishing returns for compression for larger
file sizes, at the cost of drastically lowering the fidelity of the output.
Data with differing amuounts of repetition and noise
appear to require different values of $\epsilon$ for the optimal fidelity/compression ratio.

In summary, both of the algorithms were able to achieve moderate additional compression ratios
on top of \Sequitur. The fidelity between the original and lossified strings
wasn't great as measured by levenshtein distance, but we think the texture in both the
rep-n and gen-n datasets was preserved well. Both fidelity and compression ratio were
very sensitive to $\epsilon$ depending on the kind of data.
The differences between the two algorithms lie almost entirely in their
runtime. Empirically the \Similarity algorithm tends to
consistently incur an overhead of about $O(\sqrt{n})$ on top of the runtime for
the \Cluster algorithm. Because of this we dismiss the \Similarity algorithm as
unscalable and do not test it further.

\subsection{Grammar Reduction Impact}

Although the grammars generated by \Sequitur are irreducible, our lossifiers
produce grammars which could potentially be compressed further (losslessly)
using Reduction Rules without changing the output string. To test these rules,
we looked at the rep-n and gen-n
test cases again, this time focusing on the clustering algorithm for $\epsilon
= .2$. We report our results in table \ref{red_t}, where
we give the ratio on top of the simplified grammar produced by
the clustering algorithm, and the time taken to run just the reduction rules.

When plotted on a log-log scale, the runtime for our straightforward
implementation of the reduction rules appears to be superpolynomial. While
modest additional gains of about 30 percent are possible for highly
self-similar data, very few additional gains are possible for more diverse
datasets such as gen-n. This appears to be because the only simplifications
that are possible on diverse datasets don't lead to the wide-ranging repetitive
substitutions which allow reduction rules to be allowed.

Because we were unable to get our implementation of
reduction rules to scale to larger datasets, we
will not consider them further.

\subsection{Larger Tests}

Though we were ultimately not able to come up with a good way to quantify how
much of the structure of a text was preserved, we have included some final
output strings obtained after running the \Cluster algorithm with $\epsilon
= .4$ in the Appendices. We believe these tests serve as a good (subjective)
summary of the strengths and weaknesses of our algorithm in compressing data
while preserving its character.

The first piece of text is from the gen-n dataset again, the first 256000
characters of the book of Genesis. As listed in table \ref{simclus_g_t}, the
compressed grammar was size 5540 compared with \Sequitur's size of 8060.
Without knowledge of English spelling, the \Cluster algorithm produces output
that is hard to read. However, key words are clearly recognizable, especially
important words such as `God', and the overall division of the book into
verses was retained.

The second piece of text is one of the entries to this year's International
Obfuscated C code contest, at \url{http://www.ioccc.org/2012/deckmyn/deckmyn.c}
.  The size of the lossified grammar is 1576 compared with the original size of
1973. The intricate whitespacing was lost, but the clear delineation into
sections was kept, key words such as `define' and `for' are very visible,
and the texture of the obfuscated C is very obvious.

\section{Conclusions}

\subsection{Summary of Results}

* General concept of simplifying grammars, two proposed algorithms
* reasonably efficient implementation with runtime better than expected
* compared algorithms, evaluated effectiveness of reducer
* looked at effect of epsilon

\subsection{Ratios and Fidelity}

* How useful are these algorithms??
* What is the ratio we get
* What kind of fidelity to we get

\subsection{Future Work}

There are many further directions to explore in the area of lossy grammar
compression.  One immediate direction for future work is to develop additional
lossifier algorithms, or to determine techniques for improving our existing
ones.  One shortcoming of our \Cluster algorithm is that, by replacing clusters
with the shortest rule, we lose both fidelity in the final expanded stream
\emph{and} structure in the simplified grammar.  Replacing each cluster with
the most frequently-appearing rule could improve this situation, at some small
cost to the final compression ratio (small because all rules in a cluster are
similar in size).  Unfortunately, we were unable to devise an efficient
algorithm to perform this sort of replacement without accidentally introducing
cycles.

Investigating the intuition---or perhaps, providing some methodology---for
choosing the parameter $\epsilon$ would also be useful.  Ideally, we could
reliably choose $\epsilon$ in such a way that we maximize the ratio of
compression to fidelity loss.  Investigating the effect that $\epsilon$ has for
additional types of structured data might shed some light on this problem.

Additional metrics for lossified grammars are also an important direction to
consider.  Ultimately, our goal is to compress the \emph{hierarchical
structure} of data, rather than the data itself, efficiently and with high
fidelity.  However, we were only able to directly measure how well we preserved
structure on contrived data, such as rep-n.  On real datasets, we instead made
a first-order approximation by comparing the fully-expanded output streams.
Some broadly-applicable metrics for comparing hierarchical structures would
significantly improve the evaluability of lossy grammar compression techniques.

\pagebreak

\nocite{*}

\bibliography{sources}

\pagebreak

\appendix

\section{Data}

\begin{table}[H]
  \centering
  \subfloat[$\epsilon = .4$]{%
  \begin{tabular}{l||l|l|l|l||l|l|l|l}
    & \multicolumn{4}{c||}{\Similarity} & \multicolumn{4}{c}{\Cluster} \\ \hline
    Dataset & Time & Ratio & Lev. & Prop. & Time & Ratio & Lev. & Prop. \\ \hline
    rep-100   & 0.006 & 18/34    & 0.713 & 0.07 & 0.002 & 20/34    & 0.535 & 0.04 \\
    rep-400   & 0.028 & 45/76    & 0.269 & 0.01 & 0.009 & 41/76    & 0.743 & 0.04 \\
    rep-1600  & 0.355 & 103/194  & 0.873 & 0.03 & 0.042 & 103/194  & 0.804 & 0.02 \\
    rep-6400  & 3.978 & 258/448  & 0.706 & 0.005& 0.203 & 256/448  & 0.869 & 0.50 \\
    rep-12800 & 44.63 & 604/1064 & 0.791 & 0.01 & 1.136 & 600/1064 & 0.856 & 0.01 \\
    rep-25600 & 135.0 & 900/1482 & 0.829 & 0.005& 2.906 & 898/1482 & 0.725 & 0.25 \\
  \end{tabular}%
  }

  \subfloat[$\epsilon = .2$]{%
  \begin{tabular}{l||l|l|l|l||l|l|l|l}
    & \multicolumn{4}{c||}{\Similarity} & \multicolumn{4}{c}{\Cluster} \\ \hline
    Dataset & Time & Ratio & Lev. & Prop. & Time & Ratio & Lev. & Prop. \\ \hline
    rep-100   & 0.002 & 30/34    & 0.040 & 0.05 & 0.002 & 30/34    & 0.040 & 0.05 \\
    rep-400   & 0.019 & 60/76    & 0.035 & 0.02 & 0.009 & 60/76    & 0.035 & 0.02 \\
    rep-1600  & 0.350 & 123/194  & 0.355 & 0.03 & 0.055 & 123/194  & 0.206 & 0.01 \\
    rep-6400  & 3.942 & 280/448  & 0.387 & 0.08 & 0.295 & 274/448  & 0.543 & 0.09 \\
    rep-12800 & 42.21 & 638/1064 & 0.491 & 0.03 & 1.105 & 632/1064 & 0.457 & 0.01 \\
    rep-25600 & 124.5 & 922/1482 & 0.565 & 0.12 & 2.597 & 926/1482 & 0.437 & 0.007 \\
  \end{tabular}%
  }
  \caption{rep-n Tests}
  \label{simclus_r_t}
\end{table}

\begin{table}[H]
  \centering
  \subfloat[$\epsilon = .4$]{%
  \begin{tabular}{l||l|l|l||l|l|l}
    & \multicolumn{3}{c||}{\Similarity} & \multicolumn{3}{c}{\Cluster} \\ \hline
    Dataset & Time & Ratio & Lev. & Time & Ratio & Lev. \\ \hline
    gen-100   & 0.006 & 87/89    & 0.04 &0.006 & 87/89    & 0.04 \\
    gen-400   & 0.175 & 224/258  & 0.12 &0.069 & 219/258  & 0.19 \\
    gen-1600  & 4.863 & 539/697  & 0.31 &0.732 & 529/697  & 0.33 \\
    gen-6400  & 216.9 & 1740/2348& 0.41 &9.127 & 1694/2348& 0.43 \\
    gen-12800 & 1812  & 3296/4523& 0.40 &29.17 & 3181/4523& 0.44 \\
    gen-25600 & 12989 & 5823/8060& 0.43 &176.7 & 5540/8060& 0.50 \\
  \end{tabular}%
  }

  \subfloat[$\epsilon = .2$]{%
  \begin{tabular}{l||l|l|l||l|l|l}
    & \multicolumn{3}{c||}{\Similarity} & \multicolumn{3}{c}{\Cluster} \\ \hline
    Dataset & Time & Ratio & Lev. & Time & Ratio & Lev. \\ \hline
    gen-100   & 0.004 & 89/89    & 0.00 &0.002 & 89/89    & 0.00 \\
    gen-400   & 0.085 & 245/258  & 0.03 &0.053 & 245/258  & 0.03 \\
    gen-1600  & 1.292 & 664/697  & 0.03 &0.874 & 664/697  & 0.03 \\
    gen-6400  & 73.83 & 2173/2348& 0.05 &14.02 & 2171/2348& 0.05 \\
    gen-12800 & 549.2 & 4197/4253& 0.05 &45.64 & 4186/4253& 0.05 \\
    gen-25600 & 3939  & 7405/8060& 0.06 &224.9 & 7374/8060& 0.06 \\
  \end{tabular}%
  }
  \caption{gen-n Tests}
  \label{simclus_g_t}
\end{table}

\begin{table}[H]
  \centering
  \subfloat[rep-n tests]{
  \begin{tabular}{l|l|l}
    Dataset & Time & Ratio \\ \hline
    rep-100   & 0.008 & 29/30 \\
    rep-400   & 0.028 & 56/60 \\
    rep-1600  & 0.132 & 96/123 \\
    rep-6400  & 1.145 & 153/186 \\
    rep-12800 & 19.43 & 403/632 \\
  \end{tabular}
  }
  \subfloat[gen-n tests]{
    \begin{tabular}{l|l|l}
    Dataset & Time & Ratio \\ \hline
    gen-100   & 0.016 & 89/89 \\
    gen-400   & 0.126 & 245/245 \\
    gen-1600  & 4.833 & 660/664 \\
    gen-6400  & 308.7 & 2139/2171 \\
    \end{tabular}
  }
  \caption{Reducer Tests for $\epsilon = .2$}
  \label{red_t}
\end{table}

\subsection*{deckmyn.c}

\subsubsection*{Input:}

\begin{lstlisting}
#include<stdio.h>
#define c(C) printf("%c",C)
#define C(c) ((int*)(C[1]+6))[c]
main(int c,char
*C[])         {(C[c]=C[
1]+2   )[0]=   c(52*c(\
'C'+  '4'/4)    );for(c
=0;  c<491;++   c)for(*
*C=  C[1]['c'    +c]  =
0;*   C[0]<8;(    **  C
)++    )C[1][c+   'c']=
*(C[  1]+c+'c')+  C[1][
99+    c]+(C[1    ][**C
+8*c  +99]==32    );  (
*C)[4]=*C[2]==    75  ?
*((C[2]+=3)-2    )==70?
1:0:0;C(0)=C(    1)=c=0
;while(*C[2]?   C[2][1]
?*(C[2]+2)?1    :0:0:0)
{if( *C [2      ]>'w'){
C(1)=0;C[1]    [2]++;*C
[2]=0;}else   C(1)+=*C[
2]==58?2+(    C[2][3]&&
*(C[2]+3)<   'x'):*C[2]
=='s'?(C[   2][1]-=48):
*C[2]>=65  ?3-(*C[2]==\
'm'?1:0)  :1;C(0)=C(1)>
C(0)?C(1  ):C(0);c+=3;*
(C+2)+=3;}printf("  %d\
          %d\n",        56+8*C(    0),80**(C[3]    ++))
         ;*C[2]=0       ;C[2]       -=c;*C[3]       =0;
        while(C[3]      [1,-         1]--){;   for(  **
        C=0     ;*      *C<          80;(**   C)++)  {C
       [2]      -=3     **           C[3];   *C[3]   ++
       =0;      *C[     3]          =**C>=  51||*   *C<
       18       ||*     *C         %8!=2?0  :255   ;c(1
       -1       );c     (*C       [3]);for(       (*C)[
       1]      =0;(     *C)[    1]<3;(*C)[1]    ++)c(*C
       [3      ]|((     *C)[  4]?**C>18&&* *C<42 ?C[1][
       42     +*(*      C+1)    +3***C]:0: **C>=  11&&*
       *C     <64?      ~C[1 ][   7***C+97 +(*C)[  1]]:
        0)   );c(       *C[3 ]++)   ;for(C (1)=0;   (C(
        2)   =C(1       ))<C (0);)   {(*C) [2]=C   [2][
        1]  -49;        c=(* C[2]<=   63); c=(*   C)[0]
        -4  *(C[        3][0 ]=105-  C[2][ c]   -7*(*(C
        [2]+c)<         'c') -18*(  C[2][c    ]<77)+2*(
         *C)[4          ]-7* (C[2] [c]<'C'  ))-6;for(C(
        3)=0;
       C(3)<                                (*C[2]?*C[2
      ]>'r'                             ?C[    2][1]:(1
     +2*(*C             [2]>   64)    +(2-!C[2    ][3])
    *(58 ==             (*(C         +2))[0])-
    (C[2] [0            ]=='m'   )  ):C(2)?C(0     )-C(
  2):0  );C(3)++)       C(1)+=c(C  [1][4]|(*     C[2]&&
 *C[2  ]<'s'?*C[2]      ==58?C(3)  ==1?**C     >17&&**C
 <51  ?C[ 2]    [1]     ==59?39:  C[2][1     ]==58?9:1:
0:0  :63  >*     C[2    ]?(c<7&&  c>-9?C[1    ][(*C[2]<
45?  'c'  *5     +2*    '%':*C[   2]<  61?570    :571)+
3*c  ]:   0)      :*C   [2]>'o'  ?**C>26&&**C<29    &&!
(*C  )[    2]     ||(   *C)[2]  ==1&&(&*C[0])       [0]
 <34  &&   31     <**   C?C(3)  <2?15+225*C(3     ):0:(
 *C)[  2]  ==     3?C   (3)<2  &&**C>22&&**C     <45&&C
 (3)<      2?     C[    1][7*  *(*C)+151+C      (3)]:0:
  7==(     *C    )[2    ]&&*  *C>26&&**C       <42&&C(3
  )<2?C    [1   ][7     ***C  +135+C(3)]       :0:*C[2]
   <'k'?    (c >-5                      &&      c<5?C(3
    )<2?C   [1][(                       (*C)     [2]<3?
     207:205)+7*                        c+C(3     )]:C[
      2][2]==46               &&        (c==-2||   c==-
            1-                          2*(*C[3]%2  ))?
            96                          :0:0)|((*C)[2
             ]?
             C(         3      )        ?C(3)< 2
             &&                        *C[3]>  7   &&
             c<           1&&c  >-24  ?8:0:*   C
             [3         ]<8&&c  >1   &&c<24    ?   ' '
        :0   :0)        |(C(3) <2     &&(  **  C
       ==66   &&        *C[3   ]>14   ||*  C[  3
       ]>12   &&        58   ==**C||   *
       C[3    ]<        2  &&**C==10   )            ?
        5*  51:                       0)|(7==  (
         *C)[                        2]?*C[3]  <
8?c>13&& c<23&&C(3)<2?C
[1][144+ 7*c+C(3)]:0:C(
3) && c< -14&&c>- 24?C[
1] [7 *c +400+C(3 )]:0:
0) :! C  (3)?**C> 21&&C
[0 ][    0]   <32
?C     [ 1 ]   [(
*   C )    [2   ] +323
   +7     **   *C ]:36
 + 1   < **   C&& '0'>
*     *C ?  C[1][
   C[ 2]  [2]+162
 + 7* **C]:0:0:0));C[1
][ 3] ++;C[2]+=3;}c(0)
;C [3 ]-=2;}*C[3]=0;}}

\end{lstlisting}

\subsection*{Output:}

\begin{lstlisting}
#include<stdio.h>
#define c(C) prinf("%c",C)
#define C(c) (in*)C[]+6))[c]
mainin c,char
*C[])     {C[c]=C
]+  )[0]=  c(52*c(\
C'  '4'/4)   );for(c
=0; c<491;   c)for(*
*  C[]+'c'  c]  =
0;*  0]<8;  **  C
)    C[]+c  'c']=
C[  ]+c+'c'  C[]+99   c]+C[  ][**C
+8*  +99]=3  );
*)[]=*C
]=  7
*C[]+=3)-  )==70?
1):;C()=C(  )=c=0
;whileC
?  2][]+
?C[]+2)?  ):0)
{if(*C [    >'w'){C()=0;C[]+  ]++*C
[2]0;else   C(1)+=C[
2]=58?2+  C
3]&&
*C
+3]  'x'):C

==''?C[  C[]+-=48):
*3]>=6  3-C
]=\
m'?10)  1;C()=C(1)>C(0)?C(  :C();c+=3;*
C[+)+=3;}prinf("  \
      \n",    56+8*C(  0),80**C
  +))
     *C3]=0    C
    -=c*C3]    0;
     whileC
  [1,-    ]+--){  for  **
    =0  ;*  *C      80;(*  C)++)  {C
    3]  -=  **      3]  C[3]  ++    0;      3]      =**>=  51||  *C<    8    ||    *C      8!=2?0  25  ;c(1    -    );        3]);for    (*)[    ]  0;      <3;(*)[]+  +)c(*C    [   ]|(      4]**18&&*C<42 ?C[]+    42   (*  C+1)  3***C]0): **  11&&    *C  64?  ~C
  7***C+97 +(*)[  ]+]:    0)  );c(     ]++  ;forC[ (1)0;  C[    +)  C(    ))<C (0);  {(*) 3]=C  ][    ]+  -49    c=(* C
<   63); c=(*   )[0]    -4  C[    3][02]105-  ][ c  -7(*C[    ][c)<    'c' -18*  C[]+    <77)2*    *C)[     ]-7* (C
 [c]<'C'  )-6;forC[    3)0;              C(3]       (C
?C

  ]>'r'     C[  C[]+:(1
  2*      ][  64   +(2-!C
  3])
  (58]=      (*C[    +))[0])-
  C[3] [0    ]='m'  ):C(+)?C(  )-C(  +)0)  ;C(3)++    C()+=c(  C[]+4]|(    3]&&*C2  ]<''?C
    =58?C(3)  ]=1?**  >17&&*C <51  C[ ]  ]+    =59?39  C[]+  2]=58?9:1:
00)  6  >    2   ?(c<7&& c>-9?C
  (*][
45  'c'    2*  '%':C[  2]  61?570  571)+
3*  ]:  0)  *C  ]['o'  **&&*C<29  &&!
(*     ]  ||  (*)[  ]=&&(&C[0]    [0] <34  &&  3   <**  ?C(3)  2?15+225*C(  ):0:(*C)[    ]=  3?  C(3)<2  &&**22&&*C  45&&C
C(3)    2?  C[  [7*  (*)+151+    3)]::
  ]=      )[2   &&*C>&&*C    <42&&C(  )<2?  [1  [7  ***C  +135+C(3)    *C3]   <'k'?  c >-          c<5?C(    2?   []+         (*C  ][3?  207:205)+7        c+C(  ):C
  ][2]]=46          c]=-2||  c]=-    1-        2*(*3]%  ))?    96      0)|((*)[       ]              *C            ?C(3)<                   ]>           c<     &&  >-  ?8):           [      <8&&  >1  &&c<       ' '      +)  +)    |(C(3) <    &&  **          ]=66           ]>1   ||*              >1        58  ]=**||                <       &&*C]=10            5*  51      0)|(7]=          (*)[      2]?C[  <     8?c>1&& c<2&&C(3)2?C
C[]+144+ 7*c+C():0:C(
3) && c< -14&&c>- 24C[
1] [7 *c +400+C( ):0:
) :!*C  3)** 2&&C
[0   0]  3
?   [ 1 ]
*   )   2    +323    +7     *C ]:36
 + 1   **  && '0'>
  *C   C
][      [ 2  ][16
 + 7* **]:):0));C

][ 3] ++C
+=3;}c()
;C [3 ]-=2;}C[]=0;}

\end{lstlisting}

\pagebreak

\subsection*{gen256000 Output String, Cluster, $\epsilon=.4$:}

\subsubsection*{Input:}

\begin{lstlisting}
1:1 In the beginning God created the heaven and the earth.

1:2 And the earth was without form, and void; and darkness was upon
the face of the deep. And the Spirit of God moved upon the face of the
waters.

1:3 And God said, Let there be light: and there was light.

1:4 And God saw the light, that it was good: and God divided the light
from the darkness.

1:5 And God called the light Day, and the darkness he called Night.
And the evening and the morning were the first day.

1:6 And God said, Let there be a firmament in the midst of the waters,
and let it divide the waters from the waters.

1:7 And God made the firmament, and divided the waters which were
under the firmament from the waters which were above the firmament:
and it was so.

1:8 And God called the firmament Heaven. And the evening and the
morning were the second day.

1:9 And God said, Let the waters under the heaven be gathered together
unto one place, and let the dry land appear: and it was so.

1:10 And God called the dry land Earth; and the gathering together of
the waters called he Seas: and God saw that it was good.

(Rest Omitted)

\end{lstlisting}

\subsubsection*{Output:}

\begin{lstlisting}
1:1 n e beininGod Hae mae ane ar

ine ars itht frm,n void;n darkness aupon
t ofe en.ine Spirt ofGod moend upont ofwate

3e said, htn e s ht

ine w ht, tht waodn God dividhtfrm darknes

5caledht Dayn darkness e caledNht
n e e frinwee e frt

6e said,  amament ine midt ofwate
ndt t dividwatewate

7ine fealn dividwatehichs in
underfeal wates hichs inabenfeal:n t wa

8caledmamenHaen.e e e
frinwee e secon

9e said, undere mae bemae e mae entoone plcen t e dry ln apart wa

0caleddry ln Earn e mae inmae  fwatecalede Sarine tht waod

(Rest Omitted)

\end{lstlisting}

\end{document}
